# ğŸ–¼ï¸ğŸ”Š Image Caption to Speech

This project demonstrates a simple yet powerful AI pipeline that performs **image captioning** and **text-to-speech (TTS)** conversion.

You provide an image â†’ the model generates a **textual description** (caption) â†’ and then the caption is **converted into speech** (audio). This project combines **vision**, **language**, and **speech** models in one workflow.

---

## ğŸ” Project Description

### âœ¨ Objective:
Extract a meaningful description from an image and convert that description into human-like speech.

### ğŸ§  How It Works:
1. **Image Captioning (BLIP model)**: An AI model looks at the image and generates a descriptive sentence.
2. **Text-to-Speech (Coqui TTS)**: The generated sentence is spoken aloud using a TTS model.

---

## ğŸ§  Models Used

### 1. ğŸ”· BLIP (Bootstrapping Language-Image Pretraining)
- **Model:** `Salesforce/blip-image-captioning-base`
- **Type:** Vision-Language Model (Image â†’ Text)
- **Use:** Generates a caption for an input image.
- **Example Output:** `"A dog sitting on green grass"`

### 2. ğŸ”Š Coqui TTS
- **Model:** `tts_models/en/ljspeech/tacotron2-DDC`
- **Type:** Neural Text-to-Speech Model (Text â†’ Audio)
- **Use:** Converts the generated caption into audio and saves it as a `.wav` file.

---

## ğŸ› ï¸ Dependencies
transformers â€“ for loading BLIP model

Pillow â€“ for image handling

TTS â€“ for text-to-speech (Coqui TTS)

torch â€“ required for running models

torchaudio â€“ TTS backend audio support

---
Output
Caption Generated: "A girl standing in front of a mountain"

Audio File: caption_audio.wav with spoken version of the caption
